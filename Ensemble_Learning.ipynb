{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "-  Ensemble Learning is a machine learning technique in which multiple individual models, called base learners,\n",
        "are combined to build a stronger and more accurate model.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of weak or moderately accurate models can work together\n",
        "to produce better predictions than any single model alone. By combining their outputs through methods such as\n",
        "voting, averaging, or weighting, ensemble models reduce errors caused by bias, variance, or noise.\n",
        "\n",
        "Common ensemble methods include Bagging, Boosting, and Random Forest.\n",
        "Ensemble learning improves model performance, robustness, and generalization on unseen data.\n"
      ],
      "metadata": {
        "id": "zivsS01yglai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Bagging and Boosting?\n",
        "-  Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques used to improve model performance,\n",
        "but they differ in how models are trained and combined.\n",
        "\n",
        "Bagging trains multiple models independently on different random subsets of the training data created using bootstrap sampling.\n",
        "All models are given equal importance, and their predictions are combined using averaging (for regression) or majority voting (for classification).\n",
        "Bagging mainly helps in reducing variance and is effective for high-variance models such as Decision Trees.\n",
        "\n",
        "Boosting trains models sequentially, where each new model focuses more on the instances that were misclassified by previous models.\n",
        "Models are weighted based on their performance, and final predictions are made using weighted voting.\n",
        "Boosting mainly helps in reducing bias and can convert weak learners into strong learners.\n",
        "\n",
        "In summary, Bagging reduces variance by parallel training, while Boosting reduces bias by sequentially correcting errors.\n"
      ],
      "metadata": {
        "id": "oPVMXwOCglWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "-  Bootstrap sampling is a statistical resampling technique in which multiple training datasets are generated\n",
        "from the original dataset by randomly selecting data points with replacement.\n",
        "Because sampling is done with replacement, some observations may appear multiple times in a bootstrap sample,\n",
        "while some may not appear at all.\n",
        "\n",
        "In Bagging methods such as Random Forest, bootstrap sampling plays a crucial role in creating diversity among models.\n",
        "Each decision tree in a Random Forest is trained on a different bootstrap sample of the data,\n",
        "which ensures that the trees are not identical.\n",
        "\n",
        "This diversity reduces the variance of the model and helps prevent overfitting.\n",
        "By aggregating predictions from many independently trained trees,\n",
        "Random Forest achieves better accuracy, stability, and generalization compared to a single decision tree.\n"
      ],
      "metadata": {
        "id": "oTvTF6dagkiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "-  Out-of-Bag (OOB) samples are the data points that are not selected during bootstrap sampling\n",
        "when training individual models in bagging-based ensemble methods such as Random Forest.\n",
        "Since bootstrap sampling is done with replacement, on average about 63% of the data is used to train each model,\n",
        "while the remaining 37% becomes Out-of-Bag samples.\n",
        "\n",
        "The OOB score is used as an internal validation method to evaluate the performance of ensemble models.\n",
        "For each data point, predictions are made using only the models for which that data point was not included\n",
        "in the training sample, and these predictions are then aggregated.\n",
        "\n",
        "The OOB score provides an unbiased estimate of model accuracy without requiring a separate validation dataset.\n",
        "This makes it an efficient and reliable evaluation technique, especially for large ensemble models like Random Forest.\n"
      ],
      "metadata": {
        "id": "xKGBFZqjgkfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- Feature importance analysis measures how much each feature contributes to the predictive power of a model.\n",
        "\n",
        "In a single Decision Tree, feature importance is calculated based on how much each feature reduces impurity\n",
        "(e.g., Gini or Entropy) at each split. The importance is derived from the single tree structure, so it can be\n",
        "biased toward features with more levels or continuous values, and it may vary significantly if the tree changes.\n",
        "\n",
        "In a Random Forest, feature importance is averaged across all the trees in the ensemble.\n",
        "Each tree is trained on a different bootstrap sample with random feature selection at each split,\n",
        "which reduces bias and provides a more stable and reliable estimate of feature importance.\n",
        "Random Forest also reduces variance compared to a single tree, making the importance scores more generalizable.\n",
        "\n",
        "In summary, while a single Decision Tree can give quick insights into feature importance, Random Forest\n",
        "provides a more robust and accurate assessment by aggregating multiple trees.\n"
      ],
      "metadata": {
        "id": "CR2id0s7gkbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to: ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "7AHWMRfCiuXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4qUelVkghGB",
        "outputId": "5f31a510-0085-45d5-d7a6-5c33957b3e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for easy visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "kILYz2mWjb-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(), # Changed 'base_estimator' to 'estimator'\n",
        "    n_estimators=50, random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print accuracy results\n",
        "print(\"Accuracy of Single Decision Tree:\", accuracy_dt)\n",
        "print(\"Accuracy of Bagging Classifier:\", accuracy_bag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOd-fNfJjrUR",
        "outputId": "8c731abb-8684-4a07-edb6-a65f02915132"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy (Include your Python code and output in the code box below.)\n",
        "-"
      ],
      "metadata": {
        "id": "78V1ak3kj45q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Train and find best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Evaluate final model\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oQqFihpkWEb",
        "outputId": "31a3f9df-b734-4c05-cc99-27d0f6160319"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE) (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "o4QVk-3ZkprT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Bagging Regressor using Decision Trees\n",
        "bagging = BaggingRegressor(n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print Mean Squared Errors\n",
        "print(\"MSE of Bagging Regressor:\", mse_bag)\n",
        "print(\"MSE of Random Forest Regressor:\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R5yTkIHk1Ek",
        "outputId": "8bcb0f7e-2f30-46e6-f74a-34addfbf30c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE of Bagging Regressor: 0.25787382250585034\n",
            "MSE of Random Forest Regressor: 0.25650512920799395\n"
          ]
        }
      ]
    }
  ]
}